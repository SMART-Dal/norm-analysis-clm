{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a36fae8-1df1-4df4-8732-b025d5b373e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, './utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc95b8d7-18f1-4a04-83c2-61900f877350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mootez/courses/s23/dir_std/code/norm-analysis-clm/norm_analysis/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json, os\n",
    "from tqdm import tqdm\n",
    "import queue\n",
    "import code_tokenize as ctok\n",
    "from transformers import AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "from itertools import chain\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946cdec1-1054-4022-8aec-cfd1d0123cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tag = \"microsoft/codebert-base\"\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f055496e-701c-4811-b8b7-2ef8b56b539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(filename, data_list):\n",
    "    \"\"\"\n",
    "    Save a list of dictionaries as JSON Lines (JSONL) format.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the output JSONL file.\n",
    "        data_list (list): List of dictionaries to be saved.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for data_dict in data_list:\n",
    "            json.dump(data_dict, file)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e351a963-5a5a-4379-8e19-b0d71e79e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path, split=''):\n",
    "    content = []\n",
    "    with open(file_path, 'r') as jsonl_file:\n",
    "    # Read the lines of the file\n",
    "        lines = jsonl_file.readlines()\n",
    "    for line in lines:\n",
    "        # Parse the JSON object\n",
    "        json_data = json.loads(line)\n",
    "        content.append(json_data)\n",
    "        # Now you can work with the JSON data\n",
    "        # For example, you can access values using keys\n",
    "        #project, commit_hash, func, target = json_data['project'], json_data['commit_id'], json_data['func'], json_data['target']\n",
    "        #content.append({'func': func, 'label': target, 'project': project, 'hash_id': commit_hash, 'split': split})\n",
    "        \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd85fc8-519b-4cf2-b638-5d199d3bd7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_consecutive_repeated_ranges(lst):\n",
    "    grouped_ranges = []\n",
    "    \n",
    "    if len(lst) == 0:\n",
    "        return grouped_ranges\n",
    "    \n",
    "    begin = 0\n",
    "    \n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] != lst[i - 1]:\n",
    "            grouped_ranges.append((begin, i - 1))\n",
    "            begin = i\n",
    "    \n",
    "    grouped_ranges.append((begin, len(lst) - 1))  # Append the last range\n",
    "    \n",
    "    return grouped_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47afbf42-2feb-4495-a7b4-508f81ced56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_indices(tokens, raw_tokens):\n",
    "    \"\"\"\n",
    "    Adapted from: Wan et al. (https://github.com/CGCL-codes/naturalcc)\n",
    "    \"\"\"\n",
    "    mask = []\n",
    "    raw_i = 0\n",
    "    collapsed = ''\n",
    "    special ='Ġ'\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "\n",
    "        while len(token) > 0 and token[0] == special:\n",
    "            token = token[1:]\n",
    "        collapsed += token\n",
    "        mask.append(raw_i)\n",
    "        if collapsed == raw_tokens[raw_i]:\n",
    "            raw_i += 1\n",
    "            collapsed = ''\n",
    "    if raw_i != len(raw_tokens):\n",
    "        raise Exception(f'Token mismatch: \\n{tokens}\\n{raw_tokens}')\n",
    "    return torch.tensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d3ab7f-b1fe-4612-91e6-a8a9cdeb54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_hf_idx(code_tokens):\n",
    "    hf_tokens = hf_tokenizer.tokenize(' '.join(code_tokens))\n",
    "    enc_plus = hf_tokenizer.encode_plus(' '.join(code_tokens))\n",
    "    sub_tokenized_ids = group_indices(hf_tokens, code_tokens)\n",
    "    grouped = group_consecutive_repeated_ranges(sub_tokenized_ids)\n",
    "    hf_mapping = []\n",
    "    idx = 0 # Because we will add [CLS] later on at the beginning\n",
    "    for i, tok in enumerate(code_tokens):\n",
    "        l = grouped[i][0]\n",
    "        u = grouped[i][1]\n",
    "        _range = (u - l)+1\n",
    "        hf_idx = []\n",
    "        for _ in range(_range):\n",
    "            idx += 1\n",
    "            hf_idx.append(idx)\n",
    "        hf_mapping.append([tok, hf_idx])\n",
    "    # Ref: https://github.com/huggingface/tokenizers/issues/266\n",
    "    # With CodeBERT's tokenizer if a token with string quotes such \"foo bar\", it is not\n",
    "    # possbile to revert its subtokenization. Hence we discard the sample if the detokenization process\n",
    "    # has failed.\n",
    "\n",
    "    # Doing assertions for sanity checks. \n",
    "    # The following lines iterates over the original set of tokens `code_tokens`\n",
    "    # and their corresponding set of huggingface tokens' IDs, and checks whether\n",
    "    # the detokenized string from these IDs matches the token from `code_tokens`\n",
    "    for i, tok in enumerate(code_tokens):\n",
    "        l_bound = hf_mapping[i][1][0]-1 # Again because of [CLS]\n",
    "        u_bound = hf_mapping[i][1][-1]\n",
    "        decoded_token = hf_tokenizer.convert_tokens_to_string(hf_tokens[l_bound: u_bound]).replace(' ','')\n",
    "        if decoded_token != tok:\n",
    "            return [], []\n",
    "    \n",
    "    return hf_mapping, hf_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4dd1dbe-7c27-4ba3-b1e2-143d1c120e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [os.path.join(\"..\", \"raw_data\", \"train.json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad0de169-30cf-4d04-862a-1a2fbf14529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(read_jsonl, data_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b37711e-fa86-4d70-b193-87ba27f3c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(chain(*data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f4000b-c14e-4bdd-a5de-7118871d3487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['length', 'code_tokens', 'code', 'feature_map'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "092f40d0-5d1b-4776-817c-82c1bfa084b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50876"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18c09481-89db-410d-9619-fa3b6b977494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid samples are those whose set of tokens generated by ctok is the same as the original code_tokens\n",
    "# The reason is because we are using ctok for token type tagging.\n",
    "valid_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4f24d7-dda1-4474-ab74-a52318073f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 50876/50876 [00:23<00:00, 2194.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for code in tqdm(data):\n",
    "    src_code = code['code']\n",
    "    ctok_tokens = []\n",
    "    try:\n",
    "        ctok_tokens = ctok.tokenize(src_code, lang=\"python\")\n",
    "        # Remove #NEWLINE#, #DEDENT# and #INDENT#\n",
    "        removable_tokens = [\"#NEWLINE#\", \"#DEDENT#\", \"#INDENT#\"]\n",
    "        ctok_tokens = [tok for tok in ctok_tokens if str(tok) not in removable_tokens]\n",
    "    except:\n",
    "        ctok_tokens = []\n",
    "    original_tokens = code['code_tokens']\n",
    "    diff = abs(len(ctok_tokens) - len(original_tokens))\n",
    "    if diff == 0:\n",
    "        valid_samples.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5152bd45-3913-42c5-871b-491dc5a32e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49380"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432bc46b-560b-4dac-81be-2373a47bb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizable_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d7c3866-e0ec-4a54-bf8f-8e1b46864487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 49380/49380 [00:33<00:00, 1456.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for sample in tqdm(valid_samples):\n",
    "    hf_mapping, hf_tokens = map_tokens_to_hf_idx(sample['code_tokens'])\n",
    "    if (not len(hf_mapping)) or (not len(hf_tokens)):\n",
    "        continue\n",
    "    else:\n",
    "        sample['hf_mapping'] = hf_mapping\n",
    "        sample['hf_tokens'] = hf_tokens\n",
    "        detokenizable_samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5139762e-de4b-427c-b6dd-78f001455cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49380"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detokenizable_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0fe7eb3-3c40-43fa-873a-3d10454f8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason is that strings with qutation marks cannot be reverted back from the set of Huggingface tokens to\n",
    "# the original set of tokens\n",
    "samples_with_no_comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1548a5cb-559e-4f34-99f4-dd83838daedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 49380/49380 [00:21<00:00, 2278.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Discard code with comments\n",
    "for code in tqdm(detokenizable_samples):\n",
    "    src_code = code['code']\n",
    "    ctok_tokens = ctok.tokenize(src_code, lang=\"python\")\n",
    "    ctok_types = set([tok.type for tok in ctok_tokens])\n",
    "    if ('line_comment' in ctok_types) or ('block_comment' in ctok_types) or ('comment' in ctok_types):\n",
    "        continue\n",
    "    else:\n",
    "        samples_with_no_comments.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da205638-a383-4268-9ea6-ad9cbb4210bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49380"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples_with_no_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34a689fd-0c34-462a-87fc-f72f363ef977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of python tokens:\n",
    "# 1) Keywords\n",
    "# 2) Identifiers\n",
    "# 3) Literals\n",
    "# 4) Operators\n",
    "# 5) symbols\n",
    "keywords = ['False', 'await', 'else', 'import', 'pass', 'None', 'break', 'except', 'in', 'raise', 'True', 'class', 'finally', 'is', 'return',\n",
    "           'and', 'continue', 'for', 'lambda', 'try', 'as', 'def', 'from', 'nonlocal', 'while', 'assert', 'del', 'global', 'not', 'with',\n",
    "           'async', 'elif', 'if', 'or', 'yield', 'not in', 'is not', 'none', 'print'] # (caveat: print was no longer a keyword in Python 3)\n",
    "operators = [\n",
    "'+', '-', '*', '**', '/', '//', '%', '@',\n",
    "    '<<', '>>', '&', '|', '^', '~', ':=',\n",
    "    '<', '>', '<=', '>=', '==', '!=', 'unary_operator'\n",
    "]\n",
    "symbols = [\"(\",\")\",\"[\",\"]\",\"{\",\"}\", \",\", \":\" ,\".\" ,\";\" ,\"=\" ,\"->\", \"+=\", \"-=\", \"*=\", \"/=\", \"//=\", \"%=\", \"@=\", \"&=\", \"|=\", \"^=\", \">>=\", \"<<=\", \"**=\", \"line_continuation\", \"ellipsis\"]\n",
    "literals = [\"float\", \"integer\", \"true\", \"false\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b605e5fe-97d5-4b0f-8138-4585e250ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_to_id = {\n",
    "    'Keyword': -1,\n",
    "    'Operator': -2,\n",
    "    'SpecialSymbol': -3,\n",
    "    'Literal': -4,\n",
    "    'Identifier': -5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f7b5b-0618-4913-8f8b-dbe3b3dfe29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████████████████████████████████████▍                                           | 28056/49380 [00:18<00:13, 1531.83it/s]"
     ]
    }
   ],
   "source": [
    "for sample in tqdm(samples_with_no_comments):\n",
    "    src_code = sample['code']\n",
    "    ctok_tokens = ctok.tokenize(src_code, lang=\"python\")\n",
    "    removable_tokens = [\"#NEWLINE#\", \"#DEDENT#\", \"#INDENT#\"]\n",
    "    ctok_tokens = [tok for tok in ctok_tokens if str(tok) not in removable_tokens]\n",
    "    token2cat = []\n",
    "    cat_ids = []\n",
    "    c_tok_idx = 0\n",
    "    for tok in ctok_tokens:\n",
    "        if tok.type in keywords:\n",
    "            token2cat.append([str(tok), \"Keyword\"])\n",
    "            cat_ids.append(-1)\n",
    "            \n",
    "        if tok.type in operators:\n",
    "            token2cat.append([str(tok), \"Operator\"])\n",
    "            cat_ids.append(-2)\n",
    "            \n",
    "        if tok.type in symbols:\n",
    "            token2cat.append([str(tok), \"SpecialSymbol\"])\n",
    "            cat_ids.append(-3)\n",
    "            \n",
    "        if tok.type in literals:\n",
    "            token2cat.append([str(tok), \"Literal\"])\n",
    "            cat_ids.append(-4)\n",
    "            \n",
    "        if 'identifier' == tok.type:\n",
    "            token2cat.append([str(tok), \"Identifier\"])\n",
    "            cat_ids.append(-5)\n",
    "        \n",
    "        c_tok_idx += 1\n",
    "            \n",
    "    sample[\"tokens\"] = token2cat\n",
    "    sample[\"cat_ids\"] = cat_ids\n",
    "    if len(token2cat) != len(ctok_tokens):\n",
    "        print(token2cat)\n",
    "        print(cat_ids)\n",
    "        break\n",
    "    assert len(token2cat) == len(ctok_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b4121-130f-4b0a-b6cd-3b491eff88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = list(filter(lambda x: len(x['hf_tokens']) <= 512, samples_with_no_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a53c48-b3b6-41b4-bd1e-cddede9e8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51995d5c-9fe9-4eae-8d15-283a68661a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sample = random.sample(sample, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038e243-9fd5-4a8e-a7dc-4e39c2d5d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(os.path.join(\"..\", \"data\", \"5k_csn_python.jsonl\"), study_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "norm_analysis",
   "language": "python",
   "name": "norm_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
